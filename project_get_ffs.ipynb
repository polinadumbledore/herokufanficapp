{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install cloudscraper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jjbxP_DN_eM",
        "outputId": "7c83dd0a-751a-4fe1-b6c0-a0e0778cb4f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cloudscraper\n",
            "  Downloading cloudscraper-1.2.64-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.7/dist-packages (from cloudscraper) (3.0.9)\n",
            "Collecting requests-toolbelt>=0.9.1\n",
            "  Downloading requests_toolbelt-0.10.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.9.2 in /usr/local/lib/python3.7/dist-packages (from cloudscraper) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (1.24.3)\n",
            "Installing collected packages: requests-toolbelt, cloudscraper\n",
            "Successfully installed cloudscraper-1.2.64 requests-toolbelt-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для очищения (на всякий случай) от htlm-тегов."
      ],
      "metadata": {
        "id": "JjEMzvyh2_SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanhtml(raw_html):\n",
        "  cleanr = re.compile('<.*?>')\n",
        "  cleantext = re.sub(cleanr, '', raw_html)\n",
        "  return cleantext"
      ],
      "metadata": {
        "id": "u9Rsi8gwaNiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тут собираем на странице автора его ник, потому что на странице с выдачей почему-то нельзя его вытащить."
      ],
      "metadata": {
        "id": "2rTtAQud3DfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_author(link):\n",
        "  scraper = cloudscraper.create_scraper()\n",
        "  response = scraper.get(link)\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "  name = soup.find('div', {'class': 'user-name'})\n",
        "  return name.text"
      ],
      "metadata": {
        "id": "vuzRV-RnNpp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Собираем ссылки на тексты + названия + ник автора и ссылку на его страницу."
      ],
      "metadata": {
        "id": "Wz87PO0b3W2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_urls(link, id_ff):\n",
        "  urls = []\n",
        "\n",
        "  scraper = cloudscraper.create_scraper()\n",
        "  response = scraper.get(link)\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "  articles = soup.find_all('div', {'class': 'js-toggle-description'})\n",
        "  for article in articles:\n",
        "    title = article.find('a', {'class': 'visit-link'})\n",
        "    ff_link = 'https://ficbook.net' + title.attrs['href']\n",
        "    title = title.text\n",
        "    author = article.find('span', {'class': 'author word-break'})\n",
        "    author = author.find('a')\n",
        "    author_link = 'https://ficbook.net' + author.attrs['href']\n",
        "    author = get_author(author_link)\n",
        "    urls.append([id_ff, title, ff_link, author, author_link])\n",
        "    id_ff += 1\n",
        "  return urls"
      ],
      "metadata": {
        "id": "nasHK6S5-ofg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Собираем текст каждого фанфика."
      ],
      "metadata": {
        "id": "8cIO8g-m3jyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text(link):\n",
        "  scraper = cloudscraper.create_scraper()\n",
        "  response = scraper.get(link)\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "  text = soup.find('div', {'id': 'content'})\n",
        "  return cleanhtml(text.text)"
      ],
      "metadata": {
        "id": "oMwRetbmSoXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re"
      ],
      "metadata": {
        "id": "kxKc1rlpEQi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cloudscraper"
      ],
      "metadata": {
        "id": "LmSWYOIpN8Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv"
      ],
      "metadata": {
        "id": "0RTT5bF8YMJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = requests.session()"
      ],
      "metadata": {
        "id": "fx9Xj3IdEReJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Записываем всю информацию в .tsv файл. Каждому фанфику присваиваем уникальный id, записываем метаинформацию о нем, а так же текст.\n",
        "\n",
        "Смотрите продолжение в файлел **project_parse_ffs.ipynb**."
      ],
      "metadata": {
        "id": "vL7T4_Xf3vVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnt_added = 0\n",
        "p = 2\n",
        "with open('ffs.tsv', 'w', encoding='utf-8') as f:\n",
        "  writer = csv.writer(f, delimiter='\\t')\n",
        "  writer.writerow(['id_ff', 'title', 'ff_link', 'author', 'author_link', 'text'])\n",
        "  while cnt_added < 30:\n",
        "    link = f\"https://ficbook.net/find?fandom_filter=any&fandom_group_id=1&pages_range=6&pages_min=&pages_max=1&statuses%5B0%5D=2&ratings%5B0%5D=5&ratings%5B1%5D=6&ratings%5B2%5D=7&transl=1&tags_exclude%5B0%5D=1692&likes_min=&likes_max=&rewards_min=&date_create_min=2022-09-22&date_create_max=2022-10-22&date_update_min=2022-09-22&date_update_max=2022-10-22&title=&sort=1&rnd=994345839&find=%D0%9D%D0%B0%D0%B9%D1%82%D0%B8!&p={p}#result\"\n",
        "    ffs = get_urls(link, cnt_added)\n",
        "    for ff in ffs:\n",
        "      text = get_text(ff[2])\n",
        "      ff.append(text)\n",
        "      writer.writerow(ff)\n",
        "      cnt_added += 1\n",
        "    p += 1\n"
      ],
      "metadata": {
        "id": "2nO0OAeWEYef"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}